"""Renders all glTF-Sample-Assets using gltf_basic_materials."""

import argparse
import dataclasses
import datetime
import json
import os
import pathlib
import socket
import subprocess


@dataclasses.dataclass
class _ModelKey:
  """name/variant pair that unique identifies a scene file in model-index.json."""
  name: str
  variant: str

  def __str__(self) -> str:
    return f'{self.name}-{self.variant}'


@dataclasses.dataclass
class _TestCase:
  """glTF-Sample-Assets scene-under-test."""
  model: _ModelKey
  asset: str


@dataclasses.dataclass
class _TestResult:
  """Artifacts generated by a test for debugging."""
  model: _ModelKey
  screenshot: pathlib.Path
  stdout: pathlib.Path
  stderr: pathlib.Path
  exit_status: int


def _get_git_head_commit() -> str:
  """Returns the HEAD SHA."""
  process = subprocess.run(
      ['git', 'rev-parse', 'HEAD'], capture_output=True, check=True)
  return process.stdout.decode()


def _build_test_cases(model_index) -> list[_TestCase]:
  """Filters model-index.json into a flat list of test cases to be run.

  Args:
    model_index: glTF-Sample-Assets model-index.json, loaded into memory

  Returns:
    List of test cases to run.
  """
  test_cases: list[_TestCase] = []
  for model in model_index:
    name = model['name']
    variants = model['variants']
    for variant in variants:
      variant_file = variants[variant]
      test_cases.append(
          _TestCase(
              model=_ModelKey(name=name, variant=variant),
              asset=('glTF-Sample-Assets/Models/' +
                     f'{name}/{variant}/{variant_file}')))
  return test_cases


def _run_test(test_case: _TestCase,
              output_path: pathlib.Path,
              program: pathlib.Path) -> _TestResult:
  """Loads and renders a glTF-Sample-Asset scene.

  Args:
    test_case: The glTF-Sample-Asset under test
    output_path: Empty directory to store test results
    program: The program under test used to render the asset under test.

  Returns:
    Artifacts generated by the test.
  """
  command = [program,
             '--frame-count', '2',
             '--screenshot-frame-number', '1',
             '--gltf-scene-asset', test_case.asset,
             '--screenshot-path', 'actual.ppm',
             '--headless']
  process = subprocess.run(
      command, cwd=output_path, capture_output=True, check=False)

  # Dump debugging information to disk for triaging after a test run
  (output_path / 'stdout.log').write_bytes(process.stdout)
  (output_path / 'stderr.log').write_bytes(process.stderr)

  return _TestResult(model=test_case.model,
                     screenshot=pathlib.Path('actual.ppm'),
                     stdout=pathlib.Path('stdout.log'),
                     stderr=pathlib.Path('stderr.log'),
                     exit_status=process.returncode)


def main():
  """Loads and renders all scenes in glTF-Sample-Assets."""
  parser = argparse.ArgumentParser()
  # TODO: help text
  parser.add_argument('--program', type=pathlib.Path, required=True)
  parser.add_argument('--model-index', type=pathlib.Path, required=True)
  parser.add_argument('--output', type=pathlib.Path, required=True)
  args = parser.parse_args()

  with args.model_index.open('r') as model_index_file:
    model_index = json.load(model_index_file)

  os.mkdir(args.output)

  # Dump some state of the test environment to be included in the report.
  with (args.output / 'meta.json').open('w') as meta_file:
    json.dump({'host': str(socket.getfqdn()),
               'datetime': str(datetime.datetime.now()),
               'sha': _get_git_head_commit()}, meta_file)

  test_cases = _build_test_cases(model_index)
  for i in range(len(test_cases)):
    test_case = test_cases[i]
    test_name = str(test_case.model)

    print(f'{i+1}/{len(test_cases)}: {test_name}')

    test_output_path = args.output / test_name
    os.mkdir(test_output_path)

    test_result = _run_test(test_case, test_output_path, args.program)

    # Dump results to make report generation easier. The report generator
    # should be able to use name-variant to join results with model-index.json
    with (test_output_path / 'result.json').open('w') as result_file:
      # TODO: common helpers between test runner and report generator???
      json.dump({'name': test_result.model.name,
                 'variant': test_result.model.variant,
                 'screenshot': str(test_result.screenshot),
                 'stdout': str(test_result.stdout),
                 'stderr': str(test_result.stderr),
                 'exit_status': test_result.exit_status}, result_file)


if __name__ == '__main__':
  main()
