"""Renders all glTF-Sample-Assets using gltf_basic_materials."""

from pathlib import Path
import json
import subprocess
import os
import shutil
import datetime
import socket
from typing import Optional
import dataclasses
import argparse


@dataclasses.dataclass
class _ModelKey:
    """name/variant pair that unique identifies a scene file in model-index.json."""
    name: str
    variant: str

    def __str__(self) -> str:
        return f'{self.name}-{self.variant}'


@dataclasses.dataclass
class _TestCase:
    """glTF-Sample-Assets scene-under-test."""
    model: _ModelKey
    asset: str


@dataclasses.dataclass
class _TestResult:
    """Artifacts generated by a test for debugging."""
    model: _ModelKey
    screenshot: Path
    stdout: Path
    stderr: Path
    exit_status: int


def _get_git_head_commit() -> str:
    """Returns the HEAD SHA"""
    process = subprocess.run(['git', 'rev-parse', 'HEAD'], capture_output=True)
    return process.stdout.decode()


def _build_test_cases(model_index: dict) -> list[_TestCase]:
    """Filters model-index.json into a flat list of test cases to be run.
    
    Args:
        model_index: glTF-Sample-Assets model-index.json, loaded into memory
    
    Return:
        List of test cases to run.
    """
    test_cases: list[_TestCase] = []
    for model in model_index:
        name = model['name']
        variants = model['variants']
        for variant in variants:
            variant_file = variants[variant]
            test_cases.append(_TestCase(
                model=_ModelKey(name=name, variant=variant),
                asset=f'glTF-Sample-Assets/Models/{name}/{variant}/{variant_file}'))
    return test_cases


def _run_test(test_case: _TestCase, output_path: Path, program: Path) -> _TestResult:
    """Loads and renders a glTF-Sample-Asset scene.

    Args:
        test_case: The glTF-Sample-Asset under test
        output_path: Empty directory to store test results
        program: The program under test used to render the asset under test.

    Return:
        Side-effects of the test
    """
    command = [
        program,
        '--frame-count', '2',
        '--screenshot-frame-number', '1',
        '--gltf-scene-asset', test_case.asset,
        '--screenshot-path', 'actual.ppm',
        '--headless']
    process = subprocess.run(command, cwd=output_path, capture_output=True)

    # Dump debugging information to disk for triaging after a test run
    (output_path / 'stdout.log').write_bytes(process.stdout)
    (output_path / 'stderr.log').write_bytes(process.stderr)
    
    return _TestResult(
        model=test_case.model,
        screenshot=Path('actual.ppm'),
        stdout=Path('stdout.log'),
        stderr=Path('stderr.log'),
        exit_status=process.returncode)


def main():
    """Loads and renders all scenes in glTF-Sample-Assets."""
    parser = argparse.ArgumentParser()
    # TODO: help text
    parser.add_argument('--program', type=Path, required=True)
    parser.add_argument('--model-index', type=Path, required=True)
    parser.add_argument('--output', type=Path, required=True)
    args = parser.parse_args()

    with args.model_index.open('r') as model_index_file:
        model_index = json.load(model_index_file)

    os.mkdir(args.output)

    # Write information about the state of the test environment to be included in the report
    with (args.output / 'meta.json').open('w') as meta_file:
        json.dump({
            'host': str(socket.getfqdn()),
            'datetime': str(datetime.datetime.now()),
            'sha': _get_git_head_commit()
        }, meta_file)

    test_cases = _build_test_cases(model_index)
    for i in range(len(test_cases)):
        test_case = test_cases[i]
        test_name = str(test_case.model)

        print(f'{i+1}/{len(test_cases)}: {test_name}')

        test_output_path = args.output / test_name
        os.mkdir(test_output_path)

        test_result = _run_test(test_case, test_output_path, args.program)

        # Dump results to make report generation easier.
        # The report generator should be able to use name+variant join results with model-index.json
        with (test_output_path / 'result.json').open('w') as result_file:
            # TODO: common helpers between test runner and report generator???
            json.dump({
                'name': test_result.model.name,
                'variant': test_result.model.variant,
                'screenshot': str(test_result.screenshot),
                'stdout': str(test_result.stdout),
                'stderr': str(test_result.stderr),
                'exit_status': test_result.exit_status
            }, result_file)


if __name__ == '__main__':
    main()
